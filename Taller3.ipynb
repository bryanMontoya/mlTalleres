{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taller3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanMontoya/mlTalleres/blob/main/Taller3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGf6IBBF0t6o"
      },
      "source": [
        "# **Regresión Logística.**\n",
        "\n",
        "* Jojhan Stiven Puerta Castaño.\n",
        "* Brayan Montoya Osorio.\n",
        "\n",
        "\n",
        "\n",
        "La regresión logística es un método de machine learning usado para realizar clasificación biclase el cual es basado en modelos. Este método requiere que los datos sean linealmente separables para su buen funcionamiento y la pertenencia a una clase u otra se representa mediante probabilidades las cuales se obtienen gracias a la función sigmoide que convierte distancias a probabilidades."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUjriUEknCrl"
      },
      "source": [
        "#Importación de librerías.\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_diabetes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb80clnUy3T3"
      },
      "source": [
        "**Punto 1: Algoritmo de gradiente descendente.**\n",
        "\n",
        "El algoritmo de gradiente descendente da la posibilidad de encontrar los parámetros óptimos que se ajustan de mejor manera a los datos, su ecuación es mostrada a continuación:\n",
        "$$\\theta_{j} = \\theta_{j} - \\eta \\sum_{i=1}^{m}(\\psi^{i} - \\phi(z)) x_{j}$$\n",
        "\n",
        "Donde $\\theta$ son los parámetros, $\\eta$ es la velocidad de aprendizaje, $\\psi$ son las etiquetas, $\\phi(z)$ es la predicción y $x$ son las características.\n",
        "\n",
        "A continuación un ejemplo de programación del algoritmo de gradiente descendente:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Sy1ce_y2E7"
      },
      "source": [
        "def derParcial(proba, labels, feature):\n",
        "  d = 0    \n",
        "  labels = np.array([labels])\n",
        "  feature = np.array([feature])\n",
        "  proba = np.array([proba])\n",
        "  for i in range(len(labels)):    \n",
        "    d = d + (labels[i] - proba[i])*feature[i]    \n",
        "  return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y7mk-mwYjRj"
      },
      "source": [
        "def graDesc(X_train, y_train, n, itera):\n",
        "  m, Tcarac = np.shape(X_train)\n",
        "  t = [random.uniform(-10, 10)]\n",
        "  for _ in range(Tcarac):\n",
        "    t.append(random.uniform(-10, 10))   # Se generan los valores aleatorios iniciales para los parámetros\n",
        "\n",
        "  for _ in range(itera):\n",
        "    for i in range(Tcarac):\n",
        "      proba = evaluarModelo(X_train, t)\n",
        "      X_train = np.array([X_train[:, i]])\n",
        "      X_train = X_train.transpose()\n",
        "      der = derParcial(proba, y_train, X_train)\n",
        "      t[i] = t[i] - n*der  \n",
        "  return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlPcwFnqylXQ"
      },
      "source": [
        "**Punto 2: Evaluar modelo.**\n",
        "\n",
        "El modelo de regresión logística se evalúa a través de función sigmoide mostrada a continuación:\n",
        "\n",
        "$$h(\\theta) = \\frac{1}{1 + e^{-\\theta^{T}x}}$$\n",
        "\n",
        "A continuación un ejemplo de programación para esta función:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuQk6tDaykgL"
      },
      "source": [
        "def evaluarModelo(featuresMatrix, parametros):\n",
        "  probaVector = []\n",
        "  for features in featuresMatrix:            \n",
        "    #Insertar un uno en la posicion cero, para poder hacer el producto punto.\n",
        "    features = np.append(10, features)       \n",
        "    try:\n",
        "      pp = np.dot(parametros, features)    \n",
        "      probaVector.append(1/(1 + math.exp((-1)*pp)))                        \n",
        "    except:\n",
        "      probaVector.append(1)\n",
        "  return probaVector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ni4gEdu0jdI"
      },
      "source": [
        "**Punto 3: Función de coste.**\n",
        "\n",
        "La función de coste para la regresión logística trata de determinar el error entre el valor estimado y el valor real en el modelo. Esta función está dada por la siguiente ecuación:\n",
        "\n",
        "$$ J(\\theta(z), y, \\theta) = - [\\sum_{i=1}^{m}(y^{(i)}\\log{\\phi(z)} + (1 - y^{i})\\log{(1- \\phi(z))}] $$\n",
        "\n",
        "A continuación un ejemplo de programación para la función de coste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXWqQ0wDwCgh"
      },
      "source": [
        "def funCoste(features, labels, proba):\n",
        "  J = 0\n",
        "  for i in range(len(proba)):\n",
        "    J = J + labels[i]*np.log(proba[i]) + (1 - labels[i])*np.log(1 - proba[i])\n",
        "  return -J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMnQff7e8Uoo"
      },
      "source": [
        "**Punto 4: AUC y curva ROC**\n",
        "\n",
        "La curva ROC y el AUC son métricas que dicen que tan bueno es un modelo haciendo uso de la TPR(Tasa de verdadera aceptación) y la FPR(Tasa de falsa aceptación), las cuales se calculan a través de la sensibilidad y especificidad.\n",
        "\n",
        "A continuación se presenta un ejemplo de programación para el cálculo de la curva ROC y el área bajo la curva AUC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAPqifEr8S9d"
      },
      "source": [
        "#Dibujar curva ROC y retornar valor del AUC\n",
        "def ROC(labels, proba):        \n",
        "    #Se ordenan los scores de mayor a menor.\n",
        "    labelsOrdenado, scores, sensibilidad, especificidad, FPR = [], [], [], [], []    \n",
        "    probaOrdenado = np.flipud(np.argsort(proba))    \n",
        "    for i in probaOrdenado:\n",
        "        labelsOrdenado.append(labels[i])\n",
        "        scores.append(proba[i])        \n",
        "    \n",
        "    #Se llenan las listas de sensibilidad y especificidad.         \n",
        "    denomSensi = labels.count(1)\n",
        "    denomEspec = labels.count(0)\n",
        "    unos, ceros = 0, denomEspec     \n",
        "    for i in range(len(labelsOrdenado)):\n",
        "        if (denomSensi == 0):\n",
        "          denomSensi = 1\n",
        "        if (denomEspec == 0):\n",
        "          denomEspec = 1\n",
        "        sensibilidad.append(unos/denomSensi)    \n",
        "        especificidad.append(ceros/denomEspec)                    \n",
        "        if (labelsOrdenado[i] == 1):\n",
        "            unos+=1   \n",
        "        elif (labelsOrdenado[i] == 0):\n",
        "            ceros-=1                             \n",
        "\n",
        "    especificidad[len(especificidad) - 1] = 0\n",
        "    #Calcular TPR\n",
        "    for i in especificidad:\n",
        "        FPR.append(1 - i)\n",
        "    TPR = sensibilidad    \n",
        "    \n",
        "    plt.scatter(FPR, TPR)\n",
        "    plt.plot(FPR, TPR)\n",
        "    plt.title(\"Curva ROC\")\n",
        "    plt.xlabel(\"FPR\")\n",
        "    plt.ylabel(\"TPR\")\n",
        "        \n",
        "    return FPR, TPR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "OKiMsOpIG3mV",
        "outputId": "f45d93e3-1047-4809-f44c-95a8543d3059"
      },
      "source": [
        "labels = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
        "proba = [0.98, 0.53, 0.53, 0.48, 0.47, 0.73, 0.61, 0.34, 0.8, 0.18]\n",
        "\n",
        "FPR, TPR = ROC(labels, proba)\n",
        "print(\"FPR: \", FPR)\n",
        "print(\"TPR: \", TPR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.19999999999999996, 0.19999999999999996, 0.4, 0.4, 0.6, 1]\n",
            "TPR:  [0.0, 0.2, 0.4, 0.6, 0.6, 0.8, 0.8, 1.0, 1.0, 1.0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSRivBX0JSPo"
      },
      "source": [
        "**Punto 5:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBARxdYJJYdW",
        "outputId": "165b9d46-0297-4d32-eedf-4ab8e8da6c80"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZlkw3x4KWHe"
      },
      "source": [
        "#Descripción: Función encargada de dividir entre train y test además de aplicar estandarización a la matriz de características.\n",
        "def estandarizar(paramEstandarizacion, features, labels, test_size):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = test_size, random_state = 4)    \n",
        "    #Estandarización Min-Max.\n",
        "    if (paramEstandarizacion == 1):\n",
        "        for feature in range(X_train.shape[1]):\n",
        "            xx = X_train[: , feature]\n",
        "            x_min = xx.min()\n",
        "            x_max = xx.max()        \n",
        "            for i in range(len(X_train)):\n",
        "                X_train[i , feature] = (X_train[i , feature] - x_min)/(x_max - x_min)  \n",
        "            for i in range(len(X_test)):\n",
        "                X_test[i , feature] = (X_test[i , feature] - x_min)/(x_max - x_min)    \n",
        "    #Estandarización Z-score.    \n",
        "    elif (paramEstandarizacion == 2):\n",
        "        for feature in range(X_train.shape[1]):\n",
        "            xx = X_train[: , feature]            \n",
        "            mean = np.mean(xx)\n",
        "            std = np.std(xx)\n",
        "            for i in range(len(X_train)):\n",
        "                X_train[i , feature] = (X_train[i , feature] - mean)/std  \n",
        "            for i in range(len(X_test)):\n",
        "                X_test[i , feature] = (X_test[i , feature] - mean)/std    \n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8TDJC7LYQTC"
      },
      "source": [
        "#Descipción: Función encargada de graficar la matriz de confusión.\n",
        "def Matriz_plt(conf_Matrix, numClases):\n",
        "  axis=np.arange(numClases+1)\n",
        "  sb.heatmap(conf_Matrix, center=0, annot=True, fmt=\".2f\",\n",
        "             cmap=\"YlGnBu\", linewidths=.5, cbar=True, \n",
        "             xticklabels=axis, yticklabels=axis);               \n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.xlabel('Predicciones')\n",
        "  plt.ylabel('Real')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daYm_Di3YZ-E"
      },
      "source": [
        "#Descripción: Función encargada de calcular las métricas de desempeño.\n",
        "def metrics(yreal, ypred, plotMatriz):\n",
        "  xx = []\n",
        "  xx.append(int(np.max(yreal)))\n",
        "  xx.append(int(np.max(ypred)))\n",
        "  numClases = max(xx)  \n",
        "  acc=0\n",
        "\n",
        "  conf_Matrix = np.zeros((numClases+1,numClases+1))\n",
        "  rec=np.zeros(numClases+1)\n",
        "  pre=np.zeros(numClases+1)\n",
        "  f1=np.zeros(numClases+1)\n",
        "  for i in range(len(yreal)):\n",
        "    y = int (ypred[i])\n",
        "    x = int (yreal[i])\n",
        "    aux = conf_Matrix[x,y]\n",
        "    conf_Matrix[x,y] = aux + 1\n",
        "  \n",
        "  if (plotMatriz):\n",
        "    Matriz_plt(conf_Matrix, numClases)\n",
        "  \n",
        "  for i in range(len(conf_Matrix)):\n",
        "    rec[i] = conf_Matrix[i,i]/np.sum(conf_Matrix[i,:])    \n",
        "  \n",
        "  for i in range(len(conf_Matrix)):\n",
        "    pre[i] = conf_Matrix[i][i]/sum(conf_Matrix[:,i])      \n",
        "  \n",
        "  acc = np.diagonal(conf_Matrix).sum()/np.sum(conf_Matrix)  \n",
        "\n",
        "  for i in range(len(rec)):\n",
        "    f1[i]=2*(pre[i]*rec[i])/(pre[i]+rec[i])    \n",
        "  \n",
        "  return acc, pre, rec, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTxeVy_UKo7l"
      },
      "source": [
        "def regresionLogistica(features, labels, paramEstandarizacion):      \n",
        "  #Estandarizar la matriz de características.\n",
        "  X_train, X_test, y_train, y_test = estandarizar(paramEstandarizacion, features, labels, 0.2)    \n",
        "  #llamado a función de gradiente descendente.      \n",
        "  parametros = graDesc(X_train, y_train, 0.01, 1000)\n",
        "  #Evaluar Modelo    \n",
        "  vectorProba = evaluarModelo(X_test, parametros)  \n",
        "  ROC(y_test, vectorProba)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}